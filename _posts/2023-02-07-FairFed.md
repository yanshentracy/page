---
layout: post
title:  "FairFed: Enabling Group Fairness in Federated Learning?"
link:  "https://arxiv.org/abs/2110.00857"
file:  
date:  2023-02-07 17:07:19
authors: Y.-H. Ezzeldin*, <strong>S. Yan*</strong>, C. He, E. Ferrara, and S. Avestimehr <strong>(co-first author)</strong>
pub: 37th AAAI Conference on Artificial Intelligence (AAAI'23)
page: 
categories: pub
---
<p><strong>Abstract:</strong> Training ML models which are fair across different demographic groups is of critical importance due to the increased integration of ML in crucial decision-making scenarios such as healthcare and recruitment. Federated learning has been viewed as a promising solution for collaboratively training machine learning models among multiple parties while maintaining the privacy of their local data. However, federated learning also poses new challenges in mitigating the potential bias against certain populations (e.g., demographic groups), as this typically requires centralized access to the sensitive information (e.g., race, gender) of each datapoint. Motivated by the importance and challenges of group fairness in federated learning, in this work, we propose FairFed, a novel algorithm for fairness-aware aggregation to enhance group fairness in federated learning. Our proposed approach is server-side and agnostic to the applied local debiasing thus allowing for flexible use of different local debiasing methods across clients. We evaluate FairFed empirically versus common baselines for fair ML and federated learning, and demonstrate that it provides fairer models particularly under highly heterogeneous data distributions across clients. We also demonstrate the benefits of FairFed in scenarios involving naturally distributed real-life data collected from different geographical locations or departments within an organization.
